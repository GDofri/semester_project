{
 "cells": [
  {
   "cell_type": "code",
   "id": "cf46c8ca6de1b101",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T16:17:21.114816Z",
     "start_time": "2024-11-10T16:17:21.088234Z"
    }
   },
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T20:37:06.922785Z",
     "start_time": "2024-11-09T20:37:04.297588Z"
    }
   },
   "source": [
    "from simple_transformer import Transformer\n",
    "from dataset import StreaksDataset\n",
    "import datasets.datasets as datasets\n",
    "import torch\n",
    "import utils"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.1.1 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/home/dofri/miniconda3/envs/new_env/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/home/dofri/miniconda3/envs/new_env/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/dofri/.local/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/dofri/.local/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/dofri/.local/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/dofri/.local/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/home/dofri/miniconda3/envs/new_env/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/home/dofri/miniconda3/envs/new_env/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/home/dofri/miniconda3/envs/new_env/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/dofri/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 542, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/home/dofri/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 531, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/home/dofri/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/home/dofri/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 359, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/home/dofri/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 775, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/home/dofri/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 446, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/home/dofri/.local/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/home/dofri/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/home/dofri/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/home/dofri/.local/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/dofri/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/home/dofri/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/home/dofri/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_191757/3907932105.py\", line 2, in <module>\n",
      "    from dataset import StreaksDataset\n",
      "  File \"/home/dofri/epfl/semester_project/models/dataset.py\", line 8, in <module>\n",
      "    from sklearn.model_selection import train_test_split\n",
      "  File \"/home/dofri/miniconda3/envs/new_env/lib/python3.10/site-packages/sklearn/__init__.py\", line 84, in <module>\n",
      "    from .base import clone\n",
      "  File \"/home/dofri/miniconda3/envs/new_env/lib/python3.10/site-packages/sklearn/base.py\", line 19, in <module>\n",
      "    from .utils._estimator_html_repr import _HTMLDocumentationLinkMixin, estimator_html_repr\n",
      "  File \"/home/dofri/miniconda3/envs/new_env/lib/python3.10/site-packages/sklearn/utils/__init__.py\", line 11, in <module>\n",
      "    from ._chunking import gen_batches, gen_even_slices\n",
      "  File \"/home/dofri/miniconda3/envs/new_env/lib/python3.10/site-packages/sklearn/utils/_chunking.py\", line 8, in <module>\n",
      "    from ._param_validation import Interval, validate_params\n",
      "  File \"/home/dofri/miniconda3/envs/new_env/lib/python3.10/site-packages/sklearn/utils/_param_validation.py\", line 11, in <module>\n",
      "    from scipy.sparse import csr_matrix, issparse\n",
      "  File \"/home/dofri/miniconda3/envs/new_env/lib/python3.10/site-packages/scipy/sparse/__init__.py\", line 295, in <module>\n",
      "    from ._csr import *\n",
      "  File \"/home/dofri/miniconda3/envs/new_env/lib/python3.10/site-packages/scipy/sparse/_csr.py\", line 11, in <module>\n",
      "    from ._sparsetools import (csr_tocsc, csr_tobsr, csr_count_blocks,\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;31mAttributeError\u001B[0m: _ARRAY_API not found"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy.core.multiarray failed to import",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msimple_transformer\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Transformer\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdataset\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m StreaksDataset\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mdatasets\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdatasets\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mdatasets\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n",
      "File \u001B[0;32m~/epfl/semester_project/models/dataset.py:8\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mrnn\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m pad_sequence\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m DataLoader, Dataset\n\u001B[0;32m----> 8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodel_selection\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m train_test_split\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mutils\u001B[39;00m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpad_sequences\u001B[39m(sequences):\n",
      "File \u001B[0;32m~/miniconda3/envs/new_env/lib/python3.10/site-packages/sklearn/__init__.py:84\u001B[0m\n\u001B[1;32m     70\u001B[0m     \u001B[38;5;66;03m# We are not importing the rest of scikit-learn during the build\u001B[39;00m\n\u001B[1;32m     71\u001B[0m     \u001B[38;5;66;03m# process, as it may not be compiled yet\u001B[39;00m\n\u001B[1;32m     72\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     78\u001B[0m     \u001B[38;5;66;03m# later is linked to the OpenMP runtime to make it possible to introspect\u001B[39;00m\n\u001B[1;32m     79\u001B[0m     \u001B[38;5;66;03m# it and importing it first would fail if the OpenMP dll cannot be found.\u001B[39;00m\n\u001B[1;32m     80\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m     81\u001B[0m         __check_build,  \u001B[38;5;66;03m# noqa: F401\u001B[39;00m\n\u001B[1;32m     82\u001B[0m         _distributor_init,  \u001B[38;5;66;03m# noqa: F401\u001B[39;00m\n\u001B[1;32m     83\u001B[0m     )\n\u001B[0;32m---> 84\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbase\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m clone\n\u001B[1;32m     85\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_show_versions\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m show_versions\n\u001B[1;32m     87\u001B[0m     __all__ \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m     88\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcalibration\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     89\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcluster\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    130\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mshow_versions\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    131\u001B[0m     ]\n",
      "File \u001B[0;32m~/miniconda3/envs/new_env/lib/python3.10/site-packages/sklearn/base.py:19\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_config\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m config_context, get_config\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexceptions\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m InconsistentVersionWarning\n\u001B[0;32m---> 19\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_estimator_html_repr\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _HTMLDocumentationLinkMixin, estimator_html_repr\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_metadata_requests\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _MetadataRequester, _routing_enabled\n\u001B[1;32m     21\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_param_validation\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m validate_parameter_constraints\n",
      "File \u001B[0;32m~/miniconda3/envs/new_env/lib/python3.10/site-packages/sklearn/utils/__init__.py:11\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _joblib, metadata_routing\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_bunch\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Bunch\n\u001B[0;32m---> 11\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_chunking\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m gen_batches, gen_even_slices\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_estimator_html_repr\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m estimator_html_repr\n\u001B[1;32m     14\u001B[0m \u001B[38;5;66;03m# Make _safe_indexing importable from here for backward compat as this particular\u001B[39;00m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;66;03m# helper is considered semi-private and typically very useful for third-party\u001B[39;00m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;66;03m# libraries that want to comply with scikit-learn's estimator API. In particular,\u001B[39;00m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;66;03m# _safe_indexing was included in our public API documentation despite the leading\u001B[39;00m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;66;03m# `_` in its name.\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/new_env/lib/python3.10/site-packages/sklearn/utils/_chunking.py:8\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_config\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m get_config\n\u001B[0;32m----> 8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_param_validation\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Interval, validate_params\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mchunk_generator\u001B[39m(gen, chunksize):\n\u001B[1;32m     12\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Chunk generator, ``gen`` into lists of length ``chunksize``. The last\u001B[39;00m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;124;03m    chunk may have a length less than ``chunksize``.\"\"\"\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/new_env/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:11\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnumbers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Integral, Real\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[0;32m---> 11\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mscipy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msparse\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m csr_matrix, issparse\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_config\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m config_context, get_config\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mvalidation\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _is_arraylike_not_scalar\n",
      "File \u001B[0;32m~/miniconda3/envs/new_env/lib/python3.10/site-packages/scipy/sparse/__init__.py:295\u001B[0m\n\u001B[1;32m    292\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mwarnings\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01m_warnings\u001B[39;00m\n\u001B[1;32m    294\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_base\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[0;32m--> 295\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_csr\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[1;32m    296\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_csc\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[1;32m    297\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_lil\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n",
      "File \u001B[0;32m~/miniconda3/envs/new_env/lib/python3.10/site-packages/scipy/sparse/_csr.py:11\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_matrix\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m spmatrix\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_base\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _spbase, sparray\n\u001B[0;32m---> 11\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_sparsetools\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (csr_tocsc, csr_tobsr, csr_count_blocks,\n\u001B[1;32m     12\u001B[0m                            get_csr_submatrix)\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_sputils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m upcast\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_compressed\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _cs_matrix\n",
      "\u001B[0;31mImportError\u001B[0m: numpy.core.multiarray failed to import"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "de50bb9bed14937",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T08:14:23.963787Z",
     "start_time": "2024-11-04T08:14:23.897811Z"
    }
   },
   "source": [
    "\n",
    "model = Transformer()\n",
    "model.eval()\n",
    "batch_size = 8\n",
    "inputs = torch.randn(batch_size, 512, 32)\n",
    "numeric_features = torch.rand(batch_size,5)\n",
    "mask = torch.ones(batch_size, 512, dtype=torch.bool)\n",
    "mask[:, 256:] = 0\n",
    "outputs = model(inputs, numeric_features, mask)\n",
    "print(outputs.shape)\n",
    "print(outputs)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1])\n",
      "tensor([[ 0.2081],\n",
      "        [ 0.0524],\n",
      "        [ 0.0327],\n",
      "        [-0.0637],\n",
      "        [-0.2525],\n",
      "        [-0.0735],\n",
      "        [ 0.1891],\n",
      "        [ 0.2999]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "execution_count": 116
  },
  {
   "cell_type": "code",
   "id": "a04b9ae70310b6e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T08:14:25.547461Z",
     "start_time": "2024-11-04T08:14:25.523675Z"
    }
   },
   "source": [
    "\n",
    "# Print the number of parameters\n",
    "num_params = sum(p.numel() for p in model.parameters())"
   ],
   "outputs": [],
   "execution_count": 117
  },
  {
   "cell_type": "code",
   "id": "924f67f5a9cf0440",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T08:14:26.972743Z",
     "start_time": "2024-11-04T08:14:26.950320Z"
    }
   },
   "source": [
    "print(num_params)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125633\n"
     ]
    }
   ],
   "execution_count": 118
  },
  {
   "cell_type": "code",
   "id": "337fdba16fe64258",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T12:32:37.734828Z",
     "start_time": "2024-11-03T12:32:37.698739Z"
    }
   },
   "source": [
    "\n",
    "# load all images\n",
    "# images_train = \n",
    "# \n",
    "# train_dataset = StreaksDataset(\n"
   ],
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'datasets' has no attribute 'split_data'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[17], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m file_names_train, file_names_val, file_names_test \u001B[38;5;241m=\u001B[39m \u001B[43mdatasets\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msplit_data\u001B[49m()\n",
      "\u001B[0;31mAttributeError\u001B[0m: module 'datasets' has no attribute 'split_data'"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "ea7831066c23cb92",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "train=0.8\n",
    "val=0.1\n",
    "test=0.1\n",
    "seed=1104\n",
    "\n",
    "# Load the data\n",
    "good_data = pd.read_csv(os.path.join(utils.get_project_root(), 'good_data.csv'))\n",
    "numerical_data_df = pd.read_csv(os.path.join(utils.get_project_root(), 'datasets/auxiliary_data.csv'))\n",
    "targets_df = pd.read_csv(os.path.join(utils.get_project_root(), 'datasets/targets.csv'))\n",
    "# Split data by filenames\n",
    "file_names = good_data['file_name'].unique()\n",
    "file_names_temp, file_names_test = train_test_split(file_names, train_size=train+val, random_state=seed)\n",
    "file_names_train, file_names_val = train_test_split(file_names_temp, train_size=train/(train+val), random_state=seed + 1)\n",
    " \n",
    "train_data = good_data[good_data['file_name'].isin(file_names_train)]\n",
    "val_data = good_data[good_data['file_name'].isin(file_names_val)]\n",
    "test_data = good_data[good_data['file_name'].isin(file_names_test)]\n",
    "\n",
    "def get_data(data):\n",
    "    images = [\n",
    "        torch.tensor(np.load(utils.get_strip_file_path(row)), dtype=torch.float) for (_, row) in data.iterrows()\n",
    "    ]\n",
    "    numeric = torch.tensor(\n",
    "        pd.merge(data, numerical_data_df, on='file_name')[numerical_data_df.columns].drop(columns=['file_name']).to_numpy()\n",
    "    )\n",
    "    targets = torch.tensor(\n",
    "        pd.merge(data.drop(columns=['ang_vel[deg/s]']), targets_df, on=['file_name', 'extension', 'ID'])['ang_vel[deg/s]'].to_numpy()\n",
    "    )\n",
    "    return images, numeric, targets\n",
    "\n",
    "print(get_data(train_data)[0][0].shape)\n",
    "\n",
    "train_dataset = StreaksDataset(*get_data(train_data))\n",
    "\n",
    "val_dataset = StreaksDataset(*get_data(val_data), \n",
    "                             images_mean=train_dataset.images_mean,\n",
    "                             images_std=train_dataset.images_std,\n",
    "                             numeric_features_mean=train_dataset.numeric_features_mean,\n",
    "                             numeric_features_std=train_dataset.numeric_features_std,\n",
    "                             targets_mean=train_dataset.targets_mean,\n",
    "                             targets_std=train_dataset.targets_std)\n",
    "test_dataset = StreaksDataset( *get_data(test_data),\n",
    "                               images_mean=train_dataset.images_mean,\n",
    "                               images_std=train_dataset.images_std,\n",
    "                               numeric_features_mean=train_dataset.numeric_features_mean,\n",
    "                               numeric_features_std=train_dataset.numeric_features_std,\n",
    "                               targets_mean=train_dataset.targets_mean,\n",
    "                               targets_std=train_dataset.targets_std)\n",
    "train_dataset.__getitem__(1)[0].shape\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T19:12:11.176789Z",
     "start_time": "2024-11-10T19:11:21.991534Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from train import train\n",
    "\n",
    "train()\n"
   ],
   "id": "664c9aa4bb52b242",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Training:   0%|          | 0/59 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric features shape:  torch.Size([16, 7])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtrain\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m train\n\u001B[0;32m----> 3\u001B[0m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/epfl/semester_project/models/train.py:51\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m()\u001B[0m\n\u001B[1;32m     48\u001B[0m     loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[1;32m     49\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m---> 51\u001B[0m     total_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     52\u001B[0m     train_loader_tqdm\u001B[38;5;241m.\u001B[39mset_postfix(loss\u001B[38;5;241m=\u001B[39mloss\u001B[38;5;241m.\u001B[39mitem())  \u001B[38;5;66;03m# Update tqdm with the current loss\u001B[39;00m\n\u001B[1;32m     54\u001B[0m avg_loss \u001B[38;5;241m=\u001B[39m total_loss \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mlen\u001B[39m(train_loader)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T15:00:56.116256Z",
     "start_time": "2024-11-14T15:00:56.030933Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformer_artifical_data import artificial_transformer\n",
    "import torch\n",
    "\n",
    "artificial_transformer = artificial_transformer.Transformer()\n",
    "\n",
    "\n",
    "data = torch.rand(8,200,32)\n",
    "mask = torch.zeros(8, 200, dtype=torch.bool)\n",
    "\n",
    "artificial_transformer.forward(data, mask)"
   ],
   "id": "816c85352e3d4e46",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before pooling: torch.Size([8, 200, 128])\n",
      "Shape after pooling: torch.Size([8, 128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0586],\n",
       "        [ 0.0529],\n",
       "        [ 0.0389],\n",
       "        [ 0.0283],\n",
       "        [ 0.0624],\n",
       "        [ 0.0603],\n",
       "        [ 0.0186],\n",
       "        [-0.0470]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5a259bbea3da68f2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
